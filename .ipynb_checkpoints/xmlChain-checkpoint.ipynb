{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import os.path\n",
    "import fnmatch\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from bs4 import Comment\n",
    "from lxml.html.soupparser import fromstring\n",
    "from lxml import etree\n",
    "from lxml.etree import tostring\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "from io import StringIO, BytesIO\n",
    "from copy import deepcopy\n",
    "import xml.dom.minidom as minidom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanAtts(soup, listAtt, name):\n",
    "    for att in listAtt:\n",
    "        for a in soup.find_all(name,{\"rend\" : att}):\n",
    "            a.attrs.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divRend(soup, listAtt, name):\n",
    "    for att in listAtt:\n",
    "        for a in soup.find_all('div',{\"rend\" : att}):\n",
    "            a.attrs.clear()\n",
    "            a.name=name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elemExtract(elems):\n",
    "    for elem in elems:\n",
    "        [x.extract() for x in xmlSoup.findAll(elem)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_tags=['p']\n",
    "def quoting(soup,listVerse,att):\n",
    "    checkQuote=False\n",
    "    for verse in listVerse:\n",
    "        for a in soup.find_all('div',{\"rend\" : verse}):\n",
    "            for tag in inv_tags: \n",
    "                for match in a.findAll(tag):\n",
    "                    match.replaceWithChildren()\n",
    "            a.attrs.clear()\n",
    "            a.name=att\n",
    "            checkQuote=True\n",
    "        if checkQuote==True:\n",
    "            print(\"+|+|+|+ Il y a quote avec le rend : \"+verse)\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_tags=['p', 'hi']\n",
    "\n",
    "def dedication(soup):\n",
    "    testDed1=False\n",
    "    testDed2=False\n",
    "    testDed3=False\n",
    "    for a in soup.find_all('quote',{\"rend\" : \"epigraphe\"}):\n",
    "        for tag in inv_tags: \n",
    "            for match in a.findAll(tag):\n",
    "                match.attrs.clear()\n",
    "                match.replaceWithChildren()\n",
    "        newTag=soup.new_tag(\"epigraph\")\n",
    "        test=re.sub(r\"\\n{2,}\",\"\\n\",a.text)\n",
    "        newTag.string=re.sub(r\"\\n\",\" ,\",test)\n",
    "        a.clear()\n",
    "        a.name=\"div\"\n",
    "        a[\"type\"]=\"dedication\"\n",
    "        a.append(newTag)\n",
    "        testDed1=True\n",
    "    if testDed1==True:\n",
    "        print(\"+.+.+.+ Il y a dédicace\")\n",
    "    for a in soup.find_all('div',{\"n\" : \"Epigraphe\"}):\n",
    "        for tag in inv_tags: \n",
    "            for match in a.findAll(tag):\n",
    "                match.attrs.clear()\n",
    "                match.replaceWithChildren()\n",
    "        newTag=soup.new_tag(\"epigraph\")\n",
    "        test=re.sub(r\"\\n{2,}\",\"\\n\",a.text)\n",
    "        newTag.string=re.sub(r\"\\n\",\" ,\",test)\n",
    "        a.clear()\n",
    "        a.name=\"div\"\n",
    "        a[\"type\"]=\"dedication\"\n",
    "        a.append(newTag)\n",
    "        testDed2=True\n",
    "    if testDed2==True:\n",
    "        print(\"+.+.+.+ Il y a dédicace2\")\n",
    "    for a in soup.find_all('div',{\"n\" : \"Dédicace\"}):\n",
    "        for tag in inv_tags: \n",
    "            for match in a.findAll(tag):\n",
    "                match.attrs.clear()\n",
    "                match.replaceWithChildren()\n",
    "        newTag=soup.new_tag(\"epigraph\")\n",
    "        test=re.sub(r\"\\n{2,}\",\"\\n\",a.text)\n",
    "        newTag.string=re.sub(r\"\\n\",\", \",test)\n",
    "        a.clear()\n",
    "        a.name=\"div\"\n",
    "        a[\"type\"]=\"dedication\"\n",
    "        a.append(newTag)\n",
    "        testDed3=True\n",
    "    if testDed3==True:\n",
    "        print(\"+.+.+.+ Il y a dédicace3\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rends={'pindent','blocktextpblocktext','frontmatter',\n",
    "         'captionmatter','realspc',\n",
    "         'calibre','calibrecalibre','sepetoile','chapter',\n",
    "         'titlechapter','pcbr','calibretitpartl','titpartl',\n",
    "        'titlesection','part','chapn','schap','dev','pre',\n",
    "        'pagecopyright','subtitlechapter','pindentinverse','pc',\n",
    "      'titchapl','linei','niv','chapno','titchapltitcenter','identauteuridentcenterc',\n",
    "      'titfblkc','identtitidentcenterc','titlblkidenteditidentcenter','startlinepictos',\n",
    "      'titchapltitleft','titchapltitjustify','encdef','c','titpartltitleft','txtcourantjustif',\n",
    "      'divprefpre','chap','sl','divautreappen','blocktextpcblocktext','t','amanuensisautosmallcaps',\n",
    "      'subtitlefrontmatter','titlefrontmatter','illustypeimagetext','captionpc','captionpcbr',\n",
    "      'pblancblocktext','pbrblocktext','blocktext','pdblocktext','blocktextpbrblocktext',\n",
    "      'ov','bold','titlinetitcenterchapno','titlinetitcenter','pgmonospacedpgheader','pd'}\n",
    "salutes={'dedicace','indentdedicaces'}\n",
    "quotes={'cita','citation',\n",
    "        'poetrypoetryintfigureadvertisementfigureadvertisement','blockquote'}\n",
    "verses={'poetrypoetryintcalibrestropint','citastroplg',\n",
    "        'poetrycontainerpoetrystanza','poemstanza',\n",
    "        'poetrycontainerpoetrystanza','poetrypoetryintstropstrop',\n",
    "        'poem','poetrystrop','poetrystropcentre'}\n",
    "toRem={'meta','dc:contributor','dc:description',\n",
    "      'dc:language','dc:identifier','dc:rights','dc:subjects',\n",
    "      'graphic','?xml-model'}\n",
    "listChap={\"chapitre\",\"Chapitre\",\"Chapitre\",\"CHAPITRE\",\n",
    "              \"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"0\"}\n",
    "listBook={\"livre\",\"Livre\",\"LIVRE\"}\n",
    "listPart={\"Partie\",\"partie\",\"PARTIE\"}\n",
    "listPreface={\"Préface\",\"Preface\",\"PREFACE\",\"PRÉFACE\",\"Préliminaire\",\"PRELIMINAIRE\",\"PRÉLIMINAIRE\"}\n",
    "sectToDel={\"propos de cette édition numérique\",\"propos de cette édition électronique\",\"START: FULL LICENSE\",\n",
    "          \"Œuvres de \",\"Page de titre\",\"Page de Titre\",\"Table des Matières\",\"Table des matières\",\n",
    "          \"TABLE DES MATIÈRES\",\"TABLE\",\"Page de Copyright\",\"Page de copyright\",\"Copyright\",\"Achevé de numériser\",\n",
    "          \"Couverture\",\"Du même auteur\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/odysseus/Bureau/ANR/code/xmlConvert/output2018_Maurice_Leblanc_Une_femme.xml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-cd3140999c48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileTemp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfnmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/odysseus/Bureau/ANR/code/xmlConvert/output'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*.xml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mfileTemp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfileTemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\":\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtei\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/odysseus/Bureau/ANR/code/xmlConvert/output'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfileTemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfileTemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#stats.write(\"\\n\"+fileTemp+\"\\n\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/odysseus/Bureau/ANR/code/xmlConvert/output2018_Maurice_Leblanc_Une_femme.xml'"
     ]
    }
   ],
   "source": [
    "ns = {'dc': 'http://purl.org/dc/elements/1.1/'}\n",
    "\n",
    "#stats = open('/home/odysseus/Bureau/Bureau/ANR/testsCode/stats.txt','w')\n",
    "\n",
    "for idx, fileTemp in enumerate(fnmatch.filter(os.listdir('/home/odysseus/Bureau/ANR/code/xmlConvert/output/'), '*.xml')):\n",
    "    fileTemp=fileTemp.replace(\"/\",\":\")\n",
    "    tei = open('/home/odysseus/Bureau/ANR/code/xmlConvert/output/'+fileTemp).read()\n",
    "    print(\"\\n\"+fileTemp)\n",
    "    #stats.write(\"\\n\"+fileTemp+\"\\n\")\n",
    "    if os.path.isfile('/home/odysseus/Bureau/ANR/code/xmlConvert/final/'+fileTemp)== False:\n",
    "        xmlSoup = soup(tei, 'html.parser')\n",
    "        for element in xmlSoup(text=lambda text: isinstance(text, Comment)):\n",
    "            element.extract()\n",
    "\n",
    "    # Nettoyage des dublin core :\n",
    "        for a in xmlSoup.find_all(\"dc:creator\"):\n",
    "            a.name=\"author\"\n",
    "            del a[\"opf:file-as\"]\n",
    "            del a[\"xmlns:dc\"]\n",
    "            del a[\"xmlns:opf\"]\n",
    "            del a[\"opf:role\"]\n",
    "        for a in xmlSoup.find_all(\"dc:date\"):\n",
    "            a.name=\"date\"\n",
    "            cutDate=a.string\n",
    "            a.string=cutDate[:4]\n",
    "            del a[\"xmlns:dc\"]\n",
    "        for a in xmlSoup.find_all(\"dc:subject\"):\n",
    "            a.name=\"subject\"\n",
    "            a.attrs.clear()\n",
    "        for a in xmlSoup.find_all(\"dc:title\"):\n",
    "            a.name=\"title\"\n",
    "            del a[\"xmlns:dc\"]\n",
    "        for a in xmlSoup.find_all(\"dc:publisher\"):\n",
    "            a.attrs.clear()\n",
    "            a.name=\"publisher\"\n",
    "\n",
    "    # italiques\n",
    "        for a in xmlSoup.find_all(\"hi\"):\n",
    "            a.attrs.clear()\n",
    "            a[\"rend\"]=\"italic\"\n",
    "        for a in xmlSoup.find_all(\"emph\"):\n",
    "            a.name=\"hi\"\n",
    "            a.attrs.clear()\n",
    "            a[\"rend\"]=\"italic\"\n",
    "\n",
    "    # clear <p>s\n",
    "        for a in xmlSoup.find_all('p'):\n",
    "            a.attrs.clear()\n",
    "            a.name=\"p\"\n",
    "\n",
    "    # quotes, citations\n",
    "        quoting(xmlSoup,quotes,\"quotecita\")\n",
    "        quoting(xmlSoup,verses,\"quoteverse\")\n",
    "\n",
    "\n",
    "    # nettoyer balise head   \n",
    "        for a in xmlSoup.find_all('head'):\n",
    "            a.attrs.clear()\n",
    "            test=a.text\n",
    "            if \"À propos de cette édition numérique\" in test:\n",
    "                a.extract()\n",
    "\n",
    "    # nettoyer les div rend\n",
    "        divRend(xmlSoup,rends,\"p\")\n",
    "        cleanAtts(xmlSoup,rends,\"div\")\n",
    "        cleanAtts(xmlSoup,rends,\"quote\")\n",
    "        cleanAtts(xmlSoup,rends,\"seg\")\n",
    "        cleanAtts(xmlSoup,rends,\"dfn\")\n",
    "        dedication(xmlSoup)\n",
    "        quoting(xmlSoup,salutes,\"salute\")\n",
    "\n",
    "        for a in xmlSoup.find_all('div',{\"rend\" : \"letter\"}):\n",
    "            a.attrs.clear()\n",
    "            a.name=\"q\"\n",
    "            a[\"type\"]=\"letter\"\n",
    "        listChap={\"chapitre\",\"Chapitre\",\"CHAPITRE\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"0\"}\n",
    "        listBook={\"livre\",\"Livre\",\"LIVRE\"}\n",
    "        listPart={\"Partie\",\"partie\",\"PARTIE\"}\n",
    "        check=False\n",
    "\n",
    "        for a in xmlSoup.find_all('div',{\"type\" : \"section\"},{\"n\":True}):\n",
    "            sectionTitle=a[\"n\"]\n",
    "            sectionTitle=sectionTitle.replace(u'\\xa0', ' ').encode('utf-8')\n",
    "            wordsInTitle=sectionTitle.decode().split(\" \")\n",
    "            for wToDel in sectToDel:\n",
    "                if wToDel in sectionTitle.decode('utf-8'):\n",
    "                    a.extract()\n",
    "            if str(sectionTitle,'utf-8')== \"À propos\" :\n",
    "                a.extract()\n",
    "            if str(sectionTitle,'utf-8')== \"Fin\" :\n",
    "                print(\"attention, il y a un chapitre appelé Fin supprimé\")\n",
    "                #stats.write(\"attention, il y a un chapitre appelé Fin supprimé\"+\"\\n\")\n",
    "                a.extract()\n",
    "\n",
    "        for a in xmlSoup.find_all('div',{\"type\" : \"section\"},{\"n\":True}):\n",
    "            sectionTitle=a[\"n\"]\n",
    "            sectionTitle=sectionTitle.replace(u'\\xa0', ' ').encode('utf-8')\n",
    "            wordsInTitle=sectionTitle.decode().split(\" \")\n",
    "            if len(set(wordsInTitle).intersection(set(listBook)))>0:\n",
    "                a.attrs.clear()\n",
    "                a.name=\"book\"\n",
    "                a[\"type\"]=\"book\"\n",
    "                a[\"title\"]=sectionTitle.decode(\"utf-8\")\n",
    "                print(\"+---+---+ Il y a des livres\")\n",
    "                #stats.write(\"+---+---+ Il y a des livres\"+\"\\n\")\n",
    "            elif len(set(wordsInTitle).intersection(set(listPart)))>0:\n",
    "                a.attrs.clear()\n",
    "                a.name=\"part\"\n",
    "                a[\"type\"]=\"part\"\n",
    "                a[\"title\"]=sectionTitle.decode(\"utf-8\")\n",
    "                print(\"+-+-+-+ Il y a des parties\")\n",
    "                #stats.write(\"+---+---+ Il y a des parties\"+\"\\n\")\n",
    "            elif len(set(wordsInTitle).intersection(set(listPreface)))>0:\n",
    "                print(\"+p+p+p+ Il y a une préface\")\n",
    "                a.attrs.clear()\n",
    "                a.name=\"div\"\n",
    "                a[\"type\"]=\"preface\"\n",
    "                a[\"title\"]=sectionTitle.decode(\"utf-8\")\n",
    "                #stats.write(\"+---+---+ Il y a une préface\"+\"\\n\")\n",
    "            elif \"Avertissement\" in wordsInTitle :\n",
    "                print(\"+p+p+p+ Il y a un avertissement\")\n",
    "                #stats.write(\"+---+---+ Il y a un avertissement\"+\"\\n\")\n",
    "                a.attrs.clear()\n",
    "                a.name=\"div\"\n",
    "                a[\"type\"]=\"avertissement\"\n",
    "                a[\"title\"]=sectionTitle.decode(\"utf-8\")\n",
    "            else :\n",
    "                a.attrs.clear()\n",
    "                a.name=\"chapter\"\n",
    "                a[\"type\"]=\"chapter\"\n",
    "                a[\"title\"]=sectionTitle.decode(\"utf-8\")\n",
    "    #             print(\"+++ Il y a des chapitres\")\n",
    "\n",
    "        for a in xmlSoup.find_all('div',{\"type\" : \"chapter\"}):\n",
    "            if a.findChild('head'):\n",
    "                a.name=\"chapter\"\n",
    "                title=a.find('head').string\n",
    "                if (title==None):\n",
    "                    a.extract()\n",
    "                else:\n",
    "                    for book in listBook:\n",
    "                        if book in title:\n",
    "                            a.name=\"book\"\n",
    "                    a.attrs.clear()\n",
    "                    a[\"title\"]=title\n",
    "\n",
    "    # inclassables\n",
    "\n",
    "        invalid_tags = ['hi', 'ref','div']\n",
    "        nbNotes=0\n",
    "        for a in xmlSoup.find_all('ref',{\"rend\" : \"renvoi\"}):\n",
    "            nbNotes+=1\n",
    "            if xmlSoup.find('div', {\"rend\":\"notecnt\"}):\n",
    "                elemTarg=xmlSoup.find('div', {\"rend\":\"notecnt\"})\n",
    "                a.name=\"note\"\n",
    "                a.attrs.clear()\n",
    "                for tag in invalid_tags: \n",
    "                    for match in elemTarg.findAll(tag):\n",
    "                        match.replaceWithChildren()\n",
    "                a.string=elemTarg.text\n",
    "                elemTarg.extract()\n",
    "\n",
    "            elif xmlSoup.findAll('dfn'):\n",
    "                listDfns=xmlSoup.findAll('dfn')\n",
    "                for dfn in listDfns:\n",
    "                    if len(dfn.findAll(\"ref\",{\"rend\":\"notenumrenvret\"}))>0:\n",
    "                        a.name=\"note\"\n",
    "                        a.attrs.clear()\n",
    "                        for tag in inv_tags: \n",
    "                            for match in dfn.findAll(tag):\n",
    "                                match.replaceWithChildren()\n",
    "                        a.string=dfn.text[1:]\n",
    "                        dfn.extract()\n",
    "                        break    \n",
    "\n",
    "\n",
    "        for a in xmlSoup.find_all('ref',{\"rend\" : \"apnb\"}):\n",
    "            nbNotes+=1\n",
    "            for elemTarg2 in xmlSoup.findAll('div', {\"rend\":\"ntb\"}):\n",
    "    #             print(\"il rentre dans la 3e condition\")\n",
    "                a.name=\"note\"\n",
    "                a.attrs.clear()\n",
    "                for tag in inv_tags: \n",
    "                    for match in elemTarg2.findAll(tag):\n",
    "                        match.replaceWithChildren()\n",
    "                a.string=re.sub(\"\\n\",\" \",elemTarg2.text)\n",
    "    #             print(a.string)\n",
    "                elemTarg2.extract()\n",
    "                break\n",
    "    #         elemTarg2.extract()\n",
    "    \n",
    "        for a in xmlSoup.find_all('ref',{\"rend\" : \"pginternal\"}):\n",
    "            nbNotes+=1\n",
    "            for elemTarg2 in xmlSoup.findAll('note'):\n",
    "    #             print(\"il rentre dans la 3e condition\")\n",
    "                a.name=\"note\"\n",
    "                a.attrs.clear()\n",
    "                for tag in inv_tags: \n",
    "                    for match in elemTarg2.findAll(tag):\n",
    "                        match.replaceWithChildren()\n",
    "                a.string=re.sub(\"\\n\",\" \",elemTarg2.text)\n",
    "    #             print(a.string)\n",
    "                elemTarg2.extract()\n",
    "                break\n",
    "        \n",
    "        if nbNotes>0:            \n",
    "            print(\"+#+#+#+ Il y a \"+str(nbNotes)+\" notes à rattacher\")\n",
    "            #stats.write(\"+#+#+#+ Il y a \"+str(nbNotes)+\" notes à rattacher\\n\")\n",
    "\n",
    "    # suppressions    \n",
    "        elemExtract(toRem)      \n",
    "        [x.extract() for x in xmlSoup.findAll('meta')]\n",
    "        [x.extract() for x in xmlSoup.findAll('table')]\n",
    "        [x.extract() for x in xmlSoup.findAll('foreign')]\n",
    "        [x.extract() for x in xmlSoup.findAll('dc:contributor')]\n",
    "        [x.extract() for x in xmlSoup.findAll('dc:description')]\n",
    "        [x.extract() for x in xmlSoup.findAll('dc:publisher')]\n",
    "        [x.extract() for x in xmlSoup.findAll('dc:language')]\n",
    "        [x.extract() for x in xmlSoup.findAll('dc:identifier')]\n",
    "        [x.extract() for x in xmlSoup.findAll('dc:rights')]\n",
    "        [x.extract() for x in xmlSoup.findAll('dc:subject')]\n",
    "        [x.extract() for x in xmlSoup.findAll('opf:meta')]\n",
    "        [x.extract() for x in xmlSoup.findAll('graphic')]\n",
    "        [x.extract() for x in xmlSoup.findAll('?xml-model')]\n",
    "        [x.extract() for x in xmlSoup.findAll('div', {\"rend\":\"illustypeimage\"})]\n",
    "        [x.extract() for x in xmlSoup.findAll('div',{'rend':\"som\"})]\n",
    "        [x.extract() for x in xmlSoup.findAll('div', {\"rend\":\"illustypeimage\"})]\n",
    "        [x.extract() for x in xmlSoup.findAll('div', {\"rend\":\"realspc\"})]\n",
    "        [x.extract() for x in xmlSoup.findAll('ref', {\"xml:id\":\"tdm\"})]\n",
    "        [x.extract() for x in xmlSoup.findAll('ref', {\"xml:id\":\"ete_1_minotaure\"})]\n",
    "        [x.extract() for x in xmlSoup.findAll('div', {\"rend\":\"pblanc\"})]\n",
    "        [x.extract() for x in xmlSoup.findAll('div', {\"rend\":\"realspcc\"})]\n",
    "        [x.extract() for x in xmlSoup.findAll('seg', {\"rend\":\"realspcc\"})]\n",
    "        [x.extract() for x in xmlSoup.findAll('div', {\"rend\":\"vertspc\"})]\n",
    "        [x.extract() for x in xmlSoup.findAll('div', {\"rend\":\"notes\"})]\n",
    "        [x.extract() for x in xmlSoup.findAll('div', {\"rend\":\"defnotes\"})]\n",
    "        \n",
    "        root=str(xmlSoup)\n",
    "\n",
    "        root=root.replace(\"xmlns=\\\"http://www.tei-c.org/ns/1.0\\\"\",\"xmlns:tei=\\\"http://www.tei-c.org/ns/1.0\\\"\")\n",
    "        root=root.replace(\"xmlns=\\\"http://www.idpf.org/2007/opf\\\"\",\"xmlns:idpf=\\\"http://www.idpf.org/2007/opf\\\"\")\n",
    "\n",
    "        myparser = etree.XMLParser(remove_blank_text=True)\n",
    "        tree   = etree.parse(StringIO(root), parser=myparser)\n",
    "\n",
    "        root=tree.getroot()\n",
    "        tei= etree.Element('TEI')\n",
    "\n",
    "    #     print(etree.tostring(tree, pretty_print=True))\n",
    "\n",
    "\n",
    "        teiHeader=etree.Element('teiHeader')\n",
    "\n",
    "        text=etree.Element('text')\n",
    "\n",
    "        back=etree.Element('back')\n",
    "        body=etree.Element('body')\n",
    "        front=etree.Element('front')\n",
    "\n",
    "        fileDesc=etree.Element('fileDesc')\n",
    "\n",
    "        titleStmt=etree.Element('titleStmt')\n",
    "        title=etree.Element('title')\n",
    "        title.text=tree.find('.//title',root.nsmap).text\n",
    "        author=etree.Element('author', \n",
    "                         attrib=OrderedDict([ \\\n",
    "                            (\"key\",\"\"), \\\n",
    "                            (\"name\",tree.find('.//author',root.nsmap).text),\\\n",
    "                            (\"from\",tree.find('.//date',root.nsmap).text),\\\n",
    "                            (\"to\",tree.find('.//date',root.nsmap).text)]))\n",
    "        attEdition = {\"n\":\"\"}\n",
    "        edition=etree.Element('edition', attrib=attEdition)\n",
    "\n",
    "        editor=etree.Element('editor',attrib=OrderedDict([(\"name\",\"\"),(\"where\",\"\")]))\n",
    "        titleStmt.append(title)\n",
    "        titleStmt.append(author)\n",
    "        titleStmt.append(edition)\n",
    "        titleStmt.append(editor)\n",
    "\n",
    "        publicationStmt=etree.Element('publicationStmt')\n",
    "        myattributes2 = {\"when\": tree.find('.//date',root.nsmap).text,\n",
    "                     \"type\": \"issued\"}\n",
    "        myattributes1 = {\"when\": tree.find('.//date',root.nsmap).text,\n",
    "                     \"type\": \"created\"}\n",
    "        date1=etree.Element('date', attrib=myattributes1)\n",
    "        date2=etree.Element('date', attrib=myattributes2)\n",
    "        publicationStmt.append(date1)\n",
    "        publicationStmt.append(date2)\n",
    "\n",
    "        editionStmt=etree.Element('editionStmt')\n",
    "        canon=\"\"\n",
    "        listSubjects=tree.findall(\".//subject\",root.nsmap)\n",
    "        keywords=etree.Element('keywords')\n",
    "        for a in listSubjects:\n",
    "            if \"canonique\" in a.text:\n",
    "                canon=\"canonique\"\n",
    "            else:\n",
    "                canon=\"non-canonique\"\n",
    "                term=etree.Element(\"term\")\n",
    "                keywords.append(term)\n",
    "        attProfDesc = {\"type\":\"\",\"tag\":canon}\n",
    "        profileDesc=etree.Element('profileDesc', attrib=OrderedDict([(\"type\",\"genre\"),(\"tag\",\"canon\")]))\n",
    "        textClass=etree.Element('textClass')\n",
    "\n",
    "        textClass.append(keywords)\n",
    "        profileDesc.append(textClass)\n",
    "        editionStmt.append(profileDesc)\n",
    "\n",
    "        fileDesc.append(titleStmt)\n",
    "        fileDesc.append(publicationStmt)\n",
    "        fileDesc.append(editionStmt)\n",
    "\n",
    "\n",
    "        titlePage=etree.Element('titlePage')\n",
    "        docAuthor=etree.Element('docAuthor')\n",
    "        docTitle=etree.Element('docTitle')\n",
    "        attTitPart1={\"main\":title.text}\n",
    "        attTitPart2={\"sub\":\"\"}\n",
    "        titlePart1=etree.Element('titlePart',attrib=attTitPart1)\n",
    "        titlePart2=etree.Element('titlePart',attrib=attTitPart2)\n",
    "        docTitle.append(titlePart1)\n",
    "        docTitle.append(titlePart2)\n",
    "        titlePage.append(docAuthor)\n",
    "        titlePage.append(docTitle)\n",
    "\n",
    "        attDed={\"type\":\"dedication\"}\n",
    "        divDed=etree.Element('div', attrib=attDed)\n",
    "        checkDed=False\n",
    "        if tree.findall('.//salute',root.nsmap):\n",
    "            salute=etree.Element('salute')\n",
    "            salute.text=tree.find('.//salute',root.nsmap).text\n",
    "            divDed.append(salute)\n",
    "            checkDed=True\n",
    "        if tree.findall('.//epigraph',root.nsmap):\n",
    "            epigraph=etree.Element('epigraph')\n",
    "            epigraph.text=tree.find('.//epigraph',root.nsmap).text\n",
    "            divDed.append(epigraph)\n",
    "            checkDed=True\n",
    "        if (checkDed==True):\n",
    "            titlePage.append(divDed)      \n",
    "\n",
    "        if tree.findall('.//div[@type=\\'preface\\']',root.nsmap):\n",
    "            attPref={\"type\":\"preface\"}\n",
    "            divPref=etree.Element('div', attrib=attPref)\n",
    "            print(\"Déplacement de la la préface\")\n",
    "            divPref=deepcopy(tree.find('.//div[@type=\\'preface\\']',root.nsmap))\n",
    "            titlePage.append(divPref)\n",
    "\n",
    "        if tree.findall('.//div[@type=\\'avertissement\\']',root.nsmap):\n",
    "            attPref={\"type\":\"preface\"}\n",
    "            divPref=etree.Element('div', attrib=attPref)\n",
    "            print(\"Déplacement de l'avertissement\")\n",
    "            divPref=deepcopy(tree.find('.//div[@type=\\'avertissement\\']',root.nsmap))\n",
    "            titlePage.append(divPref)\n",
    "\n",
    "        front.append(titlePage)\n",
    "\n",
    "\n",
    "        head=etree.Element('head')\n",
    "\n",
    "        listBooks=tree.findall(\".//subject\",root.nsmap)\n",
    "        book=etree.Element('div', \n",
    "                       attrib=OrderedDict(\n",
    "            [(\"type\",\"book\"),(\"title\",\"\"),(\"level\",\"1\")]))\n",
    "        listBook=tree.findall(\".//book\",root.nsmap)\n",
    "        listPart=tree.findall(\".//part\",root.nsmap)\n",
    "        if len(listPart)>0:\n",
    "            nbPart=0\n",
    "            print(\"Il y a \"+str(len(listPart))+\" parties\")\n",
    "            #stats.write(\"Il y a \"+str(len(listPart))+\" parties\\n\")\n",
    "            for part in listPart:\n",
    "                nbPart+=1\n",
    "                nbChapInPart=0\n",
    "                for chap in part.itersiblings(preceding=False):\n",
    "                    if chap.tag != \"part\" and chap.tag == \"chapter\":\n",
    "                        part.append(chap)\n",
    "                    else:\n",
    "                        break\n",
    "                    nbChapInPart+=1\n",
    "                if nbChapInPart>0:\n",
    "                    print(\"Il y a \"+str(nbChapInPart)+\" chapitres dans la partie \"+str(nbPart))\n",
    "                    #stats.write(\"Il y a \"+str(nbChapInPart)+\" chapitres dans la partie \"+str(nbPart)+\"\\n\")\n",
    "                else:\n",
    "                    print(\"Nombre de chapitres internes indéterminé (texte sale)\")\n",
    "                    #stats.write(\"Nombre de chapitres internes indéterminé (texte sale)\\n\")\n",
    "                body.append(part)\n",
    "        if len(listBook)>0:\n",
    "            print(\"Il y a \"+str(len(listBook))+\" livres\")\n",
    "            #stats.write(\"Il y a \"+str(len(listBook))+\" livres\\n\")\n",
    "            nbBook=0\n",
    "            for book in listBook:\n",
    "                nbBook+=1\n",
    "                nbChapInBook=0\n",
    "                for chap in book.itersiblings(preceding=False):\n",
    "                    nbChapInBook+=1\n",
    "                    if chap.tag != \"book\" and (chap.tag == \"chapter\" or chap.tag == \"part\"):\n",
    "                        book.append(chap)\n",
    "                    else:\n",
    "                        break\n",
    "                if nbChapInBook>0:\n",
    "                    print(\"Il y a \"+str(nbChapInBook)+\" chapitres dans la partie \"+str(nbBook))\n",
    "                    #stats.write(\"Il y a \"+str(nbChapInBook)+\" chapitres dans la partie \"+str(nbBook)+\"\\n\")\n",
    "                else:\n",
    "                    print(\"Nombre de chapitres internes indéterminé (texte sale)\")\n",
    "                    #stats.write(\"Il y a \"+str(nbChapInBook)+\" chapitres dans la partie \"+str(nbBook)+\"\\n\")\n",
    "                body.append(book)\n",
    "        else :\n",
    "            listChap=tree.findall(\".//chapter\",root.nsmap)\n",
    "            for chap in listChap:\n",
    "                body.append(chap)\n",
    "            if len(listChap)>0:\n",
    "                print(\"Il y a \"+str(len(listChap))+\" chapitres\")\n",
    "                #stats.write(\"Il y a \"+str(len(listChap))+\" chapitres\\n\")\n",
    "            listSect=tree.findall(\".//UndefinedSection\",root.nsmap)\n",
    "            for sect in listSect:\n",
    "                body.append(sect)\n",
    "                \n",
    "        body.append(head)\n",
    "        \n",
    "#         print(etree.tostring(body, pretty_print=True,encoding = \"unicode\"))\n",
    "        \n",
    "        text.append(front)\n",
    "        text.append(body)\n",
    "        text.append(back)\n",
    "        tei.append(teiHeader)\n",
    "        tei.append(text)\n",
    "        teiHeader.append(fileDesc)\n",
    "        \n",
    "#         print(etree.tostring(tei, pretty_print=True,encoding = \"unicode\"))\n",
    "\n",
    "        for quoteCit in tei.findall(\".//quotecita\", root.nsmap):\n",
    "    #         parag=etree.Element('p')\n",
    "    #         quoteTest=etree.Element('quote')\n",
    "    #         parag.text=etree.tostring(quoteCit, pretty_print=True)\n",
    "    #         quoteTest.append(parag)\n",
    "            quoteCit.tag=\"quote\"\n",
    "    #         print(quoteCit.tag)\n",
    "        listVerse=tei.findall(\".//quoteverse\",root.nsmap)\n",
    "        for quoteVer in listVerse:\n",
    "            parag=etree.Element('q')\n",
    "            if quoteVer.iterdescendants():\n",
    "                subChildren=quoteVer.iterdescendants()\n",
    "                for element in subChildren:\n",
    "                    if element.tag==\"l\":\n",
    "                        element.tag=\"q\"\n",
    "                    parag.append(element)\n",
    "            quoteVer.tag=\"quote\"\n",
    "            quoteVer.append(parag)\n",
    "            quoteVer.text=\"\"\n",
    "\n",
    "        for bad in tei.xpath('.//p/*[contains(.,\"Gutenberg\")]'):\n",
    "            bad.getparent().remove(bad)\n",
    "            print(\"XXXXXXXXXXXXXXXXXXXXXXXXX je rentre dans la condition Gutenberg XXXXXXXXXXXXXXXXXXXXXXXXXXX\")\n",
    "\n",
    "        for latestChap in tei.findall(\".//chapter\", root.nsmap):\n",
    "            latestChap.tag=\"div\"\n",
    "\n",
    "        for latestBook in tei.findall(\".//book\", root.nsmap):\n",
    "            latestBook.tag=\"div\"\n",
    "\n",
    "        for latestPart in tei.findall(\".//part\", root.nsmap):\n",
    "            latestPart.tag=\"div\"\n",
    "\n",
    "\n",
    "        final=str(etree.tostring(tei, pretty_print=True,encoding = \"unicode\"))\n",
    "        final = re.sub(r'ns[0-9]+:', '', final)\n",
    "        final= final.replace(\"&#10;\",\"\")\n",
    "        final= final.replace(\"●\",\"\")\n",
    "        final= final.replace(\"■\",\"\")\n",
    "        final= final.replace(\"◗\",\"\")\n",
    "        final= re.sub(r'\\n<dfn>', '', final)\n",
    "        final= re.sub(r'<dfn>', '', final)\n",
    "        final= re.sub('</dfn>', '', final)\n",
    "\n",
    "        pattern=re.compile(\"\\s+\")\n",
    "        soupFinal = soup(final, \"xml\")\n",
    "        for p in soupFinal.find_all('p'):\n",
    "            if p.string and pattern.match(p.string):\n",
    "                newtag = soupFinal.new_tag('lb')\n",
    "                p.replace_with(newtag)\n",
    "\n",
    "        f = open('./final/'+fileTemp, 'w')\n",
    "        reparsed = minidom.parseString(final)\n",
    "        f.write(soupFinal.prettify())\n",
    "        f.close()\n",
    "#stats.close()\n",
    "print(\"Fin du nettoyage XML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import OrderedDict\n",
    "import regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree as ET\n",
    "import lxml.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_ns_prefix(tree):\n",
    "    query = \"descendant-or-self::*[namespace-uri()!='']\"\n",
    "    for element in tree.xpath(query):\n",
    "        element.tag = ET.QName(element).localname\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_xml(root):\n",
    "    if root.getchildren() is not None:\n",
    "        for child in root.getchildren():\n",
    "            if child.text is None and child.getchildren() is None:\n",
    "                root.remove(child)\n",
    "            else:\n",
    "                recursive_xml(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_from_unicode(unicode_str):\n",
    "    s = unicode_str.encode('utf-8')\n",
    "    return ET.fromstring(s, parser=utf8_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags(html, invalid_tags):\n",
    "    soup = BeautifulSoup(html, \"xml\")\n",
    "    for tag in invalid_tags: \n",
    "        for match in soup.findAll(tag):\n",
    "            match.replaceWithChildren()\n",
    "    cleaned_text=str(soup)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1967', 'Simenon-Georges', 'Le-voleur-de-Maigret']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/odysseus/.local/lib/python3.6/site-packages/ipykernel_launcher.py:73: FutureWarning: The behavior of this method will change in future versions. Use specific 'len(elem)' or 'elem is not None' test instead.\n",
      "/home/odysseus/.local/lib/python3.6/site-packages/ipykernel_launcher.py:82: FutureWarning: The behavior of this method will change in future versions. Use specific 'len(elem)' or 'elem is not None' test instead.\n",
      "/home/odysseus/.local/lib/python3.6/site-packages/ipykernel_launcher.py:174: FutureWarning: The behavior of this method will change in future versions. Use specific 'len(elem)' or 'elem is not None' test instead.\n",
      "/home/odysseus/.local/lib/python3.6/site-packages/ipykernel_launcher.py:187: FutureWarning: The behavior of this method will change in future versions. Use specific 'len(elem)' or 'elem is not None' test instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1966', 'Simenon-Georges', 'Maigret-et-l-affaire-Nahour']\n",
      "['1968', 'Simenon-Georges', 'Maigret-a-Vichy']\n",
      "['1964', 'Simenon-Georges', 'Maigret-se-defend']\n",
      "['1970', 'Simenon-Georges', 'Maigret-et-le-marchand-de-vin']\n",
      "['1900', 'Eekhoud-Georges', 'La-faneuse-d-amour']\n",
      "['1968', 'Simenon-Georges', 'Maigret-hesite']\n"
     ]
    }
   ],
   "source": [
    "source_dir=\"/home/odysseus/Bureau/ANR/epub_pour_test/\"\n",
    "files= glob.iglob(source_dir+'/*.xml', recursive=True)\n",
    "#print(files)\n",
    "\n",
    "# Parsing\n",
    "\n",
    "for filename in files :\n",
    "    #print(filename)\n",
    "    basename=(filename[:-4])\n",
    "    infos=re.split(\"_\",basename[basename.rfind(\"/\")+1:])\n",
    "    print(infos)\n",
    "    \n",
    "    text=\"\"\n",
    "    \n",
    "    with open(filename, 'r') as reader:\n",
    "        text=reader.read()\n",
    "    #if \"bibebook\" in text or \"Bibebook\" in text:\n",
    "        #print(\"Texte Bibebook : \"+filename)\n",
    "        \n",
    "    text=re.sub('\\s*xml:id=\\\"[A-Za-z0-9\\-_:]+\\\"', '', text)\n",
    "    text=re.sub('<meta xml:id=\\\"[A-Za-z\\-: 0-9_]*\\\"\\s*(content=\")*[A-Za-z\\s,\\'.(0-9\\-)&;:{}]*(\"/>)*\\n','',text)\n",
    "    text=re.sub('<meta xml:id=\"[A-Za-z\\-: 0-9_]*\"/>\\n', '', text)\n",
    "    text=re.sub('xsi:type=\"[A-Za-z0-9:]*\"', '', text)\n",
    "    text=re.sub(' xml:id=\\\"[a-zA-Z0-9]*\\\"','',text)\n",
    "    text=re.sub(\"(\\n*\\s*<seg>\\n*\\s*)q(\\n*\\s*</seg>\\n*\\s*)\",\"\",text)\n",
    "    \n",
    "    #pattern = re.compile(\"((\\n*\\s*<hi rend=\\\"italic\\\">\\n*\\s*)*(\\n*\\s*<seg>\\n*\\s*))([B-Z])((\\n*\\s*</seg>\\n*\\s*)(\\n*\\s*</hi>\\n*\\s*)*)\")\n",
    "    #results = pattern.findall(text)\n",
    "    '''\n",
    "    if results is not None:\n",
    "        for result in results:\n",
    "            print(results)\n",
    "            print(result[1])\n",
    "            #print(result)\n",
    "            print(\"\".join(result))\n",
    "            text=re.sub(\"\".join(result),result[1],text)\n",
    "    print(text)\n",
    "    '''\n",
    "    \n",
    "    text=remove_tags(text, [\"seg\",\"hi\"])\n",
    "    #print(repr(text))\n",
    "    text=re.sub(\"(?!>)(\\n)+\",\"\",text)\n",
    "    text=re.sub(\"\\s{2,}\",\" \",text)\n",
    "    \n",
    "    pattern=re.compile(\"(\\s)([B-Z])(\\s)([A-ZÈÉ]+)\")\n",
    "    results=pattern.findall(text)\n",
    "    if results is not None:\n",
    "        for result in results:\n",
    "            #print(results)\n",
    "            text=re.sub(\"\".join(result),result[1]+result[3],text)\n",
    "    \n",
    "    #print(text)\n",
    "    \n",
    "    utf8_parser = ET.XMLParser(encoding='utf-8',remove_blank_text=True, resolve_entities=False, ns_clean=True, dtd_validation=False)\n",
    "    tree=parse_from_unicode(text)\n",
    "    \n",
    "    tree=strip_ns_prefix(tree)\n",
    "    \n",
    "    ps = tree.findall(\".//p\")\n",
    "    hs = tree.findall(\".//head\")\n",
    "    cs = tree.xpath('//comment()')\n",
    "    divs = tree.findall('.//div')\n",
    "    header = tree.find ('.//teiHeader')\n",
    "    body = tree.find('.//body')\n",
    "    notes = tree.findall('.//note')\n",
    "    parent=body.getparent()\n",
    "    front=ET.Element(\"front\")\n",
    "    back=ET.Element(\"back\")\n",
    "    titlePage=ET.Element(\"titlePage\")\n",
    "    titlePage.append(ET.Element(\"docAuthor\"))\n",
    "    docTitle=ET.Element(\"docTitle\")    \n",
    "                               \n",
    "    if tree.find('.//front'):\n",
    "        front=tree.find('.//front')\n",
    "    else:\n",
    "        front.append(titlePage)\n",
    "        docTitle.append(ET.Element('titlePart',attrib=OrderedDict([(\"main\",\"\")])))\n",
    "        docTitle.append(ET.Element('titlePart',attrib=OrderedDict([(\"sub\",\"\")])))\n",
    "        titlePage.append(docTitle)\n",
    "    parent.insert(parent.index(body)-1,front)\n",
    "        \n",
    "    if tree.find ('.//back'):\n",
    "        back=tree.find('.//back')\n",
    "    parent.insert(parent.index(body)+1,back)        \n",
    "    \n",
    "# Nettoyage du texte\n",
    "    \n",
    "    for p in ps:\n",
    "        rawtext= lxml.html.tostring(p, method=\"text\", encoding=\"utf8\")\n",
    "        rawtext=rawtext.decode(\"utf8\")\n",
    "        \n",
    "        pattern=\"[mle]{2,}\\n*\\s*(</[seghi]+>)*\"\n",
    "        if re.search(pattern,rawtext):\n",
    "            rawtext=re.sub(\"\\n\\s+\",\" \",rawtext)\n",
    "            rawtext=re.sub(\"M\\s\",\"M\",rawtext)\n",
    "        \n",
    "        if rawtext!=None:\n",
    "            raw_p=ET.Element(\"p\")\n",
    "            raw_p.text=rawtext\n",
    "            p.addnext(raw_p)\n",
    "            p.getparent().remove(p)\n",
    "        \n",
    "    for h in hs:\n",
    "        rawtext= lxml.html.tostring(h, method=\"text\", encoding=\"utf8\")\n",
    "        if rawtext!=None:\n",
    "            raw_h=ET.Element(\"head\")\n",
    "            raw_h.text=h.text\n",
    "            h.addnext(raw_h)\n",
    "            h.getparent().remove(h)\n",
    "    for c in cs :\n",
    "        if c.getparent() is not None:\n",
    "            c.getparent().remove(c)\n",
    "    for note in notes :\n",
    "        if note.getparent() is not None:\n",
    "            note.getparent().remove(note)\n",
    "    \n",
    "    for div in divs:\n",
    "        ET.strip_attributes(div,'rend')\n",
    "        if 'type' in div.attrib:\n",
    "            if div.attrib['type']=='section':\n",
    "                div.attrib['type']='chapter'\n",
    "            if \"n\" in div.attrib :\n",
    "                div.attrib['title']=div.attrib['n']\n",
    "                del div.attrib['n']\n",
    "                \n",
    "    \n",
    "# Reconstitution du Header  \n",
    "\n",
    "    fileDesc=ET.Element('fileDesc')\n",
    "    titleStmt=ET.Element('titleStmt')\n",
    "    title=ET.Element('title')\n",
    "    \n",
    "    if len(tree.xpath('.//title/text()'))>0 :\n",
    "        title.text=tree.xpath('.//title/text()')[0]\n",
    "    else:\n",
    "        title.text=re.sub(\"-\",\" \",infos[2])\n",
    "        \n",
    "    if len(tree.xpath('.//author/text()'))>0 :\n",
    "        name = tree.xpath('.//author/text()')[0]\n",
    "    else:\n",
    "        name=infos[1]\n",
    "    if len(tree.xpath('.//date/text()'))>0:\n",
    "        date = tree.xpath('.//date/text()')[0]\n",
    "    else:\n",
    "        date=infos[0]\n",
    "    \n",
    "    if len(tree.xpath(\".//author[@name]\"))>0:\n",
    "        author=tree.find(\".//author\")      \n",
    "    else :\n",
    "        author=ET.Element('author', \n",
    "                         attrib=OrderedDict([ \\\n",
    "                            (\"key\",\"\"), \\\n",
    "                            (\"name\",name),\\\n",
    "                            (\"from\",date),\\\n",
    "                            (\"to\",date)]))\n",
    "    \n",
    "    if tree.xpath(\".//edition[@n]\"):\n",
    "        #print(tree.xpath(\".//edition[@n]\"))\n",
    "        edition=tree.find(\".//edition\")\n",
    "    else:\n",
    "        attEdition = {\"n\":\"\"}\n",
    "        edition=ET.Element('edition', attrib=attEdition)\n",
    "\n",
    "    if tree.xpath(\".//editor[@name]\"):\n",
    "        editor=tree.find(\".//editor\")\n",
    "    else:\n",
    "        editor=ET.Element('editor',attrib=OrderedDict([(\"name\",\"\"),(\"where\",\"\")]))\n",
    "        \n",
    "    titleStmt.append(title)\n",
    "    titleStmt.append(author)\n",
    "    titleStmt.append(edition)\n",
    "    titleStmt.append(editor)\n",
    "\n",
    "    if tree.find(\".//publicationStmt\"):\n",
    "        publicationStmt=tree.find(\".//publicationStmt\")\n",
    "    else:\n",
    "        publicationStmt=ET.Element('publicationStmt')\n",
    "        myattributes2 = {\"when\": date,\n",
    "                     \"type\": \"issued\"}\n",
    "        myattributes1 = {\"when\": date,\n",
    "                     \"type\": \"created\"}\n",
    "        date1=ET.Element('date', attrib=myattributes1)\n",
    "        date2=ET.Element('date', attrib=myattributes2)\n",
    "        publicationStmt.append(date1)\n",
    "        publicationStmt.append(date2)\n",
    "\n",
    "    if tree.find(\".//editionStmt\"):\n",
    "        editionStmt=tree.find(\".//editionStmt\")\n",
    "    else:\n",
    "        editionStmt=ET.Element('editionStmt')\n",
    "        canon=\"\"\n",
    "        listSubjects=tree.findall(\".//subject\")\n",
    "        keywords=ET.Element('keywords')\n",
    "        for a in listSubjects:\n",
    "            if \"canonique\" in a.text:\n",
    "                canon=\"canonique\"\n",
    "            else:\n",
    "                canon=\"non-canonique\"\n",
    "                term=ET.Element(\"term\")\n",
    "                keywords.append(term)\n",
    "        attProfDesc = {\"type\":\"\",\"tag\":canon}\n",
    "        profileDesc=ET.Element('profileDesc', attrib=OrderedDict([(\"type\",\"genre\"),(\"tag\",\"canon\")]))\n",
    "        textClass=ET.Element('textClass')\n",
    "\n",
    "        textClass.append(keywords)\n",
    "        profileDesc.append(textClass)\n",
    "        editionStmt.append(profileDesc)\n",
    "\n",
    "    fileDesc.append(titleStmt)\n",
    "    fileDesc.append(publicationStmt)\n",
    "    fileDesc.append(editionStmt)\n",
    "    \n",
    "    #noises = header.getchildren()\n",
    "    if header is not None:\n",
    "        noises = header.getchildren()\n",
    "        for noise in noises:\n",
    "            noise.getparent().remove(noise)\n",
    "    \n",
    "    header.append(fileDesc)\n",
    "    \n",
    "    #tree.write(filename[:-4]+'_remastered.xml', pretty_print=True, encoding='utf8')\n",
    "    base=tree.getroottree()\n",
    "    base.write(source_dir+'remastered/'+basename[basename.rfind(\"/\")+1:]+\".xml\", pretty_print=True, encoding='utf8')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
